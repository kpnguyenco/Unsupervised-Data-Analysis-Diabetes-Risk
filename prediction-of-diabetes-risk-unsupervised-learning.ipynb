{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8004095,"sourceType":"datasetVersion","datasetId":4713778},{"sourceId":8266438,"sourceType":"datasetVersion","datasetId":4907343}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Diabetes Risk: Predicting Diabetes Prevalence Through Unsupervised Learning\n---","metadata":{}},{"cell_type":"markdown","source":"### 1 Introduction\n\n---\n#### 1.1 Overview\n\nDiabetes is a metabolical disorder that impacts individual's blood sugar levels. This is often due to the inability to produce or utilize insulin. Dibetes has become quite prevalent in the modern world, given the changes in diets, the prevelance of other diseases, and and many other possible external factors. However, based on known and presumed risks for diabetes, it would be useful to know if these types of risks can be detected to predict whether one is proned to having diabetes or not. To understand this topic further, it is essential to apply unsupervised learning, via clustering models, such as K-Means Clustering and Hierarchical Clustering.\n\n\n#### 1.2 Goal\n\nAs an individual who has predisposition to diabetes via genetics, it is essential to understand the potential risk that may predict diabetes, as well as understand the risks that come from having diabetes. By being able to group and identify those proned to diabetes, based on potential diabetes risks, it will enable for better awareness and long term preventative care. To approach this problem, it is essential to evaluate different clustering approaches, to determine what model would best predict diabetes, given the presence of certain factors. The main clustering approaches that will be investigated in this will be Hierarchical Clustering via Agglomerative Clustering and K-Means Clustering.","metadata":{}},{"cell_type":"markdown","source":"#### 1.3 About the Data\n\n**1.3.1 Load The Data**","metadata":{}},{"cell_type":"code","source":"import numpy as np # for statistical analysis\nimport pandas as pd # to data process and load csv.\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, learning_curve\nimport copy # to copy data as needed\nimport matplotlib.pyplot as plt # to create plots\nimport seaborn as sns # to visualize pairplots and correalation matrices\nfrom sklearn.decomposition import PCA, NMF # for non-negative matrix\nfrom sklearn.cluster import KMeans, AgglomerativeClustering # for both clustering methods\nfrom sklearn.linear_model import LogisticRegression # to create logistic regressions\nfrom sklearn.tree import DecisionTreeClassifier # to create a decision tree\nfrom sklearn.metrics import accuracy_score, precision_score, confusion_matrix # to calculate for a confusion matrix\nfrom sklearn.tree import plot_tree # to plot decision tree\nfrom sklearn.metrics import silhouette_score # eo evaluate simlarity between clusters\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# acquire the diabetes data\ndiabetes = pd.read_csv(\"/kaggle/input/diabetes-risk-prediction/diabetes_risk_prediction_dataset.csv\")\n\n# print out top 5 data sets from diabetes\ndiabetes.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print number of columns\nprint(\"Number of Columns:\", \n      len(diabetes.columns))\n\n#print number of rows\nprint(\"Number of Rows:\", \n      len(diabetes))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** Just from a glance, this data evidently showcases that every variable is a binary classification, except for `Age`. Given that the main focus is determining how risk factors can predict diabetes, it is essential to allocate `class` as the response variable. This is because `class` is classifies of whether an individual has diabetes or not, will be the response factor for the multi-linear regression. Another thing to take note of is that every variable is non-numeric, meaning all the binary classification columns will need to be converted to binary values. There is also evidently 17 features and 520 observations.","metadata":{}},{"cell_type":"markdown","source":"**1.3.2 Data Source and Citation**\n\n**_Citation:_**\nHimanshu (rcratos).(n.d.). Diabetes Risk Prediction [Data set]. Kaggle. Retrieved from https://www.kaggle.com/datasets/rcratos/diabetes-risk-prediction\n\nThe dataset that was used is called the Diabetes Risk Prediction from Kaggle. It can be found from the [Data Risk Prediction](https://www.kaggle.com/datasets/rcratos/diabetes-risk-prediction/data) page, which is cited in the link above. In this data, it provides information on individuals with different potential risk factors for diabetes, which could be used to decide risk predictions. \n\n**1.3.3 Data Description**\n\nThis data overall provides information on individuals and whether they have acquired the potential risk factors of diabetes, as well as whether they have diabetes themselves.\n\nThis data consists of is 520 rows and 7 columns. Here are all the features of this data and what they are:\n\n- **Age**: The age of the individual. (Numeric)\n- **Gender**: The gender of the individual. (Categorical: Male or Female)\n- **Polyuria**: A common diabetes symptom in which an individual urinates excessively. (Binary Classification: Yes or No)\n- **Polydipsia**: A common diabetes symptom in which an individual is excessively thirsty. (Binary Classification: Yes or No)\n- **Sudden weight loss**: A diabetes symptom when an individuals experiences severe unexplained weight loss, which can be a sign of diabetes. (Binary Classification: Yes or No)\n- **Weakness**: A sign of the individual experiences physical weakness \n- **Polyphagia**: A common diabetes symptom in which an individual is excessively hungry. (Binary Classification: Yes or No)\n- **Genital thrush**: A yeast infection that causes irritation in the genital area. (Binary Classification: Yes or No)\n- **Visual blurring**: A loss or blur in vision (Binary Classification: Yes or No)\n- **Itching**: Whether an individual experiences itching or irritation. (Binary Classification: Yes or No)\n- **Irritability**: Whether the individual experiences irritability or not. (Binary Classification: Yes or No)\n- **Delayed healing**: Whether the individual experiences slow healing of wounds.(Binary Classification: Yes or No)\n- **Partial paresis**: A symptom of diabetes in which there is partial loss of voluntary movement. (Binary Classification: Yes or No)\n- **Muscle stiffness**: Whether the individual experiences muscle stiffness. (Binary Classification: Yes or No)\n- **Alopecia**: Whether the individual suffers from hair loss. (Binary Classification: Yes or No)\n- **Obesity**: Whether the individual is obese or not (Binary Classification: Yes or No)\n- **Class**: Whether the individual has diabetes or not (Binary Classification: Yes or No)\n\nThis data will be used to evaluate vital symptoms/risks that contribute to diabetes, as well as predict causes of diabetes risk/risk factors that come from diabetes.\n\n\n### 2 Data Cleaning\n\n--- \n#### 2.1 Data Exploration\n\nThe first part of the exploratory data analysis, is to determine what kind of data is being used in this investigation. It is also important to evaluate any important parts of the data that may need to be processed. For this first part, it would be good to make sure the data reflects the data type they are described to be. Then it will be useful to know whether there are null values that may need to be preprocessed before analysis. Given that clustering and dimenstionality reduction will be modeled, it is essential to evaluate normalizing the data. The reason is it will accomodate for equal contribution from all the features, especially for evaluating distance metrics in clustering and reducing dimensions with Non-Negative Matrix.\n\n**2.1.1: Types of Usable Data**","metadata":{}},{"cell_type":"code","source":"# print out top 5 data sets from diabetes\ndiabetes.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** Based on this table, there is evidently categorical data that need to be converted to binary values, especially when it comes to any feature aside from \"Age\".","metadata":{}},{"cell_type":"code","source":"# print information on the datasets\n# check for null values\nprint(\"Diabetes Information\")\nprint(diabetes.info(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** Based on the `Diabetes Information` above, it is evident that age is the only numeric variable present in the data, which means every other column needs to be converted into binary values.\n\n**2.1.2: Evaluate Potential Need To Remove Null Values**","metadata":{}},{"cell_type":"code","source":"# print number sum of null values present in each column\nprint(\"Diabetes Information: Null Values\")\nprint(diabetes.isnull().sum(), \"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** For the `Diabetes Information: Null Values` table, there are no null variables to extract, which means this data has been processed to not include null values.\n\n**2.1.3: Evaluate Potential Need For Normalization**","metadata":{}},{"cell_type":"code","source":"# visualize box plots for \"age\" relative to class before normalization\nsns.boxplot(x = \"class\", y = \"Age\", \n            data = diabetes, palette = [\"dodgerblue\", \"r\"])\nplt.xlabel(\"Class\")\nplt.ylabel(\"Age\")\nplt.title(\"Boxplot of Age by Class Before Normalization\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** According to the boxplot, despite having different medians between classes, the boxplot size seems to look the same size, suggesting it might already be normalized.\n\n**Suggestions for Pre-Processing:** The data evalution suggests there are no null values to remove from the data for any features. However, the data itself suggests that data need to be converted to binary values before conducting exploratory data analysis. The boxplot for `Age` suggest there may not be a need to standardize the `Age` feature, but  for safe measures, it will still be best to normalize the data.","metadata":{}},{"cell_type":"markdown","source":"#### 2.2 Pre-Process Data for Exploratory Analysis\n\n**2.2.1 Approach** \n\nTo get started in further understanding that data at hand, it is vital to ensure the data is useable for the process. The first step is to convert all binary categorical labels into values. The following will be done to the labels: {`Yes = 1`, `No = 0`}, {`Male = 1; Female = 0`}, and {`Positive = 1; Negative = 0`}. Furthermore, it is essential to normalize the data, in case it has not been. From the looks of the Boxplot before, it is possible this data is already standardized for analysis. The last thing is to ensure the data is reduced enough to where only important features are referenced for non-negative factorization to avoid the curse of dimensionality.\n\n**2.2.2 Reassigning Categorical Data to Binary Values** ","metadata":{}},{"cell_type":"code","source":"# acquire the binary categorical variables and convert to numeric\n# select all yes or no binary classifications; not age, gender, and class\nbi_col = diabetes.columns.drop([\"Age\", \"Gender\", \"class\"])\n# create new diabetes dataframe with new values\ndb = diabetes.copy()\n# ensure yes = 1; no = 0\ndb[bi_col] = db[bi_col].apply(lambda x: x.map({\"Yes\": 1,\n                                               \"No\": 0}))\n# ensure male = 1; female = 0\ndb[\"Gender\"] = db[\"Gender\"].map({\"Male\": 1,\n                                 \"Female\": 0})\n# ensure positive = 1; negative = 0\ndb[\"class\"] = db[\"class\"].map({\"Positive\": 1,\n                               \"Negative\": 0})\n# make sure the new data no longer has the labels\ndb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**2.2.3 Normalizing the Data** \n\nTo ensure that the data will be usable for clustering and dimension reduction, normalizing the dataset will still be conducted.","metadata":{}},{"cell_type":"code","source":"# isolate age\nage = [\"Age\"]\n\n# conduct Standardization using StandardScalar()\nscaler = StandardScaler()\n# reassign the normalized data into the age column\ndb[age] = scaler.fit_transform(db[age])\n# display db\ndb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize box plots for \"age\" relative to class before normalization\nsns.boxplot(x = \"class\", y = \"Age\", \n            data = db, palette = [\"dodgerblue\", \"r\"])\nplt.xlabel(\"Class\")\nplt.ylabel(\"Age\")\nplt.title(\"Boxplot of Age by Class After Normalization\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** From comparing the looks of both the before and normalized data, it appears that the data was shifted due to the normalization of the age column. From looking at the boxplot, the median of the non-diabetic boxplot shifted down, while also reducing the error.\n\n**2.2.3 Data Reduction for Higher Dimensionality** \n\nFor the clustering approach, the approach of using the full standardized data alone is useful because it can model clusters based on all the varibales; however it might perform more efficiently with less data. Therefore, both clustering will undergo model analysis for both data sets, in order to find which features model the clustering better. As for dimensionality, reducing the data size can avoid the curse of dimensionality. Given NMF will be used to reduce the dimensionality beforehand, it is overall a better idea to ustilize PCA and reduce it.","metadata":{}},{"cell_type":"code","source":"# apply PCA where the number of dimensions exceed .95 of the data\npca = PCA(n_components = 0.95)  \npca.fit(db)\n\n# transform db into principal components\npca_db = pca.transform(db)\n\n# acquire components of each principal component\npca_comp = pca.components_\n\n# create DataFrame for pca\nfeat_imp = pd.DataFrame(pca_comp.T,\n                        columns=[f'PC{i+1}' for i in range(pca.n_components_)])\n\n# select features to remove based on their importance in principal components\nfeat_remove = feat_imp[feat_imp.abs().max(axis = 1) < 0.1].index\n\n# reduce db dataset\ndb_reduced = db.drop(columns = feat_remove)\n\n# display reduced data\ndb_reduced","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** Despite conducting PCA to reduce the dataset, it seems that the data has already undergone data reduction. The dimensions are also the same as it intially was in the scaled `db`. Ths means all the features are useful for both the clustering and dimenstionality reduction models. However, given the negative state of age, it is possible that it will mess up the non-negative matrix. The best thing to do from here is remove the age column.","metadata":{}},{"cell_type":"code","source":"db1 = db_reduced.drop(columns = [\"Age\"])\ndb1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2.3 Data Cleaning Overview\n\n**Summary:** Overall, there was no null data points and the data was cleaned beforehand in that aspect. However, the data values were in strings for classification labels, so before proceeding to Exploratory Data Analysis, it was necessary to convert all the Yes or No; Male or Female, and Postive or Negative binary classifications into binary values. This will help allow for correlation matrices and any other exploratory analysis to occur. Data reduction was also conducted; however it seems that the data ideally has all the important features to keep. \n\n### 3 Exploratory Data Analysis (EDA)\nIn this portion, visualizations and analysis will be conducted in order to understand the data more and simplify the data further before modeling. The first part is to find correlations between the features, as well as have and idea of what kind of relationship each chosen feature has in relation to the diabetes classification.\n\n#### 3.1 Data Statistics\nThis will allows for the understanding the demographics of who was being observed in this data set. This should not influence the prediction model but rather evaluate if the models later one can predict, even if the given demographics are different.\n\n**3.1.1 Summary Statistics**","metadata":{}},{"cell_type":"code","source":"summary_stats = db1.describe()\nprint(summary_stats)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**3.1.1 Pie Chart: Viualization of Summary Statistics**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Find every column feature, except \"Age\"\nbi_col = [col for col in db1.columns]\n\n# Create subplots with a layout of 4 rows and 4 columns\nfig, axes = plt.subplots(4, 4, figsize=(15, 10))\n\n# iterate through each column feature to create a pie chart\nfor i, col in enumerate(bi_col):\n    # within the figure dimension\n    dim = axes[i//4, i%4]\n    # get count of each plot by class\n    class_counts = db[col].value_counts()\n    \n    # Plot the pie chart\n    dim.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', colors=[\"dodgerblue\", \"r\"])\n    dim.set_title(f\"Precentage of {col.capitalize()} by Class\")\n    dim.legend(title = \"Class\", \n               loc = \"upper right\", \n               labels=[\"Negative\", \"Positive\"], \n               prop={\"size\": 8})\n        \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** Based on the summary and the pie chart visualization, this demonstrates the overall make-up of the observations in the data set. The composition of each treat overall that the majority of the individuals in the dataset have traits of : \n- **Gender**: 63.1% Male; 36.9% Female = Male\n- **Polyuria**: 50.4% Yes; 49.6% No = No Polyuria\n- **Polydipsia**: 55.2% Yes; 44.8% No = No Polydipsia\n- **Sudden weight loss**: 58.3% Yes; 41.7% No = No sudden weight loss\n- **Weakness**: 58.7% Yes; 41.3% No = No sudden weight loss\n- **Polyphagia**: 54.4% Yes; 45.6% No = No Polyphagia\n- **Genital thrush**: 77.7% Yes; 22.3% No = No genital irritation from yeast infection\n- **Visual blurring**: 55.2% Yes; 44.8% No = No Polyphagia\n- **Itching**: 51.3% Yes; 48.7% No = No itching\n- **Irritability**: 75.8% Yes; 24.2% No = No irritability\n- **Delayed healing**: 54.0% Yes; 46.0% No = No slow healing\n- **Partial paresis**: 56.9% Yes; 43.1% No = No loss of movement\n- **Muscle stiffness**: 62.5% Yes; 37.5% No = No muscle stiffness\n- **Alopecia**: 65.6% Yes; 34.4% No = No hair loss\n- **Obesity**: 83.1% Yes; 16.9% No = No obesity\n- **Class**: 61.5% Yes; 38.5% No = No diabetes.\n\n**Observations:** Since the dataset in general does not demonstrate the individuals selected to be mainly diabetic or those with risk of diabetes, this suggests the data is not biased towards selecting diabetic individual and the diabete risks. Although this does not appear to favor diabetes, it shows that the data potentially can help categorize the data in modeling more accurately without bias.\n\nTo further evaluate this, it is best to see the breakdown of each feature in respect to `class`.","metadata":{}},{"cell_type":"markdown","source":"#### 3.2 Evaluate Diabetic Risks by Class \n\n**3.2.1 Bar Pot Comparisons**\n\nIt is uncertain whether the composition of the data will effect the model, so it is best to identify which feature trait of risk is found in those with diabetes. Use barplot to distinguish each binary category to each class.","metadata":{}},{"cell_type":"code","source":"# conduct count plot for each binary category; exclude age\n# find every column feature, except age\nbi_col = [col for col in db1.columns]\nbi_col\n\n# visualize bar plots for binary variables, relative to class\nfig, axes = plt.subplots(4, 4, \n                         figsize=(15, 10))\n\n# iterate through each column feature to creat bar plot\nfor i, col in enumerate(bi_col): # exclude age\n    # within the figure dimension\n    dim = axes[i//4, i%4]\n    # creat count plot by class\n    sns.countplot(x = col, hue = \"class\", \n                  data = db, ax = axes[i//4, i%4], \n                  palette = [\"dodgerblue\", \"r\"])\n    dim.set_title(f\"Distribution of {col.capitalize()} by Class'\")\n    \nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** Based bar plot visualization, this demonstrates the overall make-up of the observations in the data set. Diabetics were more likely to: \n- **Gender (Key: Male = 1, Female = 0)**: Be Female\n- **Polyuria (Key: Yes = 1, No = 0)**: Have Polyuria\n- **Polydipsia (Key: Yes = 1, No = 0)**: Have Polydipsia\n- **Sudden weight loss (Key: Yes = 1, No = 0)**: Have sudden weight loss\n- **Weakness**: 58.7% Yes; 41.3% No = No sudden weight loss\n- **Polyphagia (Key: Yes = 1, No = 0)**: Have Polyphagia\n- **Genital thrush (Key: Yes = 1, No = 0)**: Have  genital irritation from yeast infection\n- **Visual blurring (Key: Yes = 1, No = 0)**: Have  Polyphagia\n- **Itching (Key: Yes = 1, No = 0)**: Have itching\n- **Irritability (Key: Yes = 1, No = 0)**: Have irritability\n- **Delayed healing (Key: Yes = 1, No = 0)**: Have slow healing\n- **Partial paresis (Key: Yes = 1, No = 0)**: Have loss of movement\n- **Muscle stiffness (Key: Yes = 1, No = 0)**: Have muscle stiffness\n- **Alopecia (Key: Yes = 1, No = 0)**: Have hair loss\n- **Obesity (Key: Yes = 1, No = 0)**: Have obesity\n\n**Class** could not be evaluated due to it being compared to itself.\n\n**Observations:** The data suggest that class can evaluate by different binary categories, in order to identify who will have diabetes and who will not. In result, modeling this via dimensional and clustering can help create a prediction utilizing this structure of categorization.","metadata":{}},{"cell_type":"markdown","source":"#### 3.3 Visualization Relationship Between Variables\n\nAlthough from class alone, it is able to provide a statistic of what diabete risks will more liekly have diabetes, it is interesting to evalute whether a relationship exists between all the features, especially considering that PCA found all the features important.","metadata":{}},{"cell_type":"markdown","source":"**3.3.1 Correlation Matrix**","metadata":{}},{"cell_type":"code","source":"# compute\ncorr = db1.corr()\n\n# create a heatmap on what variables are correlated to diabetes based on gender/age\nplt.figure(figsize=(10, 8))\n# create heatmap from sns library\nsns.heatmap(corr, annot = True, # include correlation values\n            cmap = \"coolwarm\", \n            square = True)\nplt.title('Diabetes Feature Correlation Matrix')\nplt.xticks(rotation = 45, \n           ha='right')\nplt.tight_layout()  # ensure the data is not too big\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** Based on the evaluation of this data, it seems the variables that have the highest correlation, relative to `class`, would be primarily `Polyuria` and `Polydipisia`. However, the lowest risks variables for having diabetes would be, `Genital thrush`, `visual blurring`, `itching`, `weakness`, `delayed healing`, `muscle stiffness`, `Alopecia`, and `Obesity`. Looking at the values present, it seems that majority of the variables, except for `Polyyuria` and `Polydipsia`, were lower than a correlation of 0.5. This indicates that class does not have a strong correlation with majority of the features, but it is worth taking notice of these two features for analysis. This does not provide any means for further processing, given PCA already ensured the data was ready for usage. To address this, it is best to utilize NMF to reduce the dimensionality instead.","metadata":{}},{"cell_type":"markdown","source":"#### 3.3 Pre-Processing Test and Train Data for Models\nNow that the target and predictor variables are understood, it is time to create prediction models based on them. However, it must first be pre-processed","metadata":{}},{"cell_type":"code","source":"# assign class as response variable, and the rest into X\nX = db1.drop(columns = [\"class\"])  # Features\ny = db1[\"class\"]  # Target variable\n\n# display X variable\nX","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display the y variable\ny ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now split data using train_test_split; make sure 20% goes to test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = 0.2, \n                                                    shuffle = True,\n                                                    random_state = 42) # for reproducibility","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3.3 EDA Overview\n\n**Summary:** Overall, the corelation matrices determined that `Age`, `class`, `gender`, `Polyuria`, `Polydipsia`, `sudden weight loss`, and `partial paresis` would be the ideal data features to analyze for modeling, and the predicted classifications of individuals to have diabetes are more likely those who are: older than 35, are female, have Polyuria, have Pollydipsia, have sudden weight loss, and have partial paresis. After simplifying the data set to a useable dataframe, the data was split into train and test data set for X and y; respectively.\n\n### 4 Modeling Analysis\n\n---\n\n#### 4.1 Reducing Dimensionality\nGiven that there are so many dimensions from the number of featurs in this data, it is possibly too high of a dimension to proceed with clustering. So it will be conducted first.","metadata":{}},{"cell_type":"code","source":"# reduce dimensionalirty with negative matrix factorization\nnmf = NMF(n_components=2, random_state=42)\nX_train_nmf = nmf.fit_transform(X_train)\nX_test_nmf = nmf.transform(X_test)\n\n# create a visualiztion of how the matrix adjusted the dimensionality\nsns.scatterplot(x = X_train_nmf[:, 0], y = X_train_nmf[:, 1], \n                hue = y_train, palette = \"Set1\", \n                legend=\"full\")\nplt.title(\"Non-Negative Matrix Factorization\")\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\nplt.legend(title=\"Class\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations** After reducing the dimensionality, it seems that the data was able to plot in a somewhat accurate model. From the plot, it demonstrates how NMF was able to trasnform the data that is more usable for plotting K-Means and Hiearchical Clustering.\n\n#### 4.2 Plot K-Means and Hierarchical Clustering\nIn this process, the K-means plot and Hierarchical Clustering Dendrogram to exhibit the underlying model for their respecitve clustering. To do the clustering, the X_train values created from NMF will be used, to ensure the transformed data helps the clustering predict better. \n\n**4.2.1 K-Means Plot**","metadata":{}},{"cell_type":"code","source":"# cluster into two groups using k_means\nkmeans = KMeans(n_clusters = 2, \n                n_init=10, \n                random_state=42)\n# use the reduces dimsionality data to predict fit\nkmeans_labels = kmeans.fit_predict(X_train_nmf)\n\n# plot for kmeans\n# first get unique labels\nuniq_labels = np.unique(kmeans_labels)\n\n# iterate through each unique label, plotting where 0 and 1's were identified\nfor i in uniq_labels:\n    plt.scatter(X_train_nmf[kmeans_labels == i, 0], X_train_nmf[kmeans_labels == i, 1], label=i)\nplt.title(\"KMeans Clustering\")\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** From the plot, it seems that the K-Means clustering was able to distinguish the data decently into two groups ; however, it can be seen that the model doesn't have enough of a definite clustering to say it is the best model for this data set. ","metadata":{}},{"cell_type":"markdown","source":"**4.2.2 Hierarchical Clustering**","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_dendrogram(model, **kwargs):\n    # acquire the children nodes ofthe model\n    children = model.children_\n    # assume distance is the number of children\n    distance = np.arange(children.shape[0])\n    # acquire the number of observations in each cluster level\n    observations = np.arange(2, children.shape[0] + 2)\n\n    # use define linkage matrix with the values above\n    linkage_matrix = np.column_stack([children, \n                                      distance, \n                                      observations]).astype(float)\n\n    # plot the dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n\n# conduct agglomerative clustering for modeling; 2 clusters, adjust linkage to highest accuracy\nagg_clustering = AgglomerativeClustering(n_clusters = 2,\n                                         distance_threshold = None,\n                                         linkage = \"complete\")\nagg_labels = agg_clustering.fit_predict(X_train_nmf)\n\n# plot the dendrogram\nplot_dendrogram(agg_clustering, truncate_mode='level', p=6)\nplt.xlabel(\"Data Points\")\nplt.ylabel(\"Distance\")\nplt.title('Agglomerative Clustering Dendrogram')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** In this Dendrogram, it is evident that there are somewhat similar groupings, but there is visually differences between data points, indicating limited similarity.\n\n#### 4.3 Evaluate Efficiency of K-Means and Hierarchical Clustering\nFrom the models before, it is evident there is some reluctance as to how well both models are able to predict the clustering; however to further ensure whether this is the case, precision, accuracy, and confusion matrices will be evaluated for both, based on the predicted permutations/label orders. Functions will be needed to evaluate true prediction values from label orders before evaluating these matrics. \n\nGiven the clustering in the K-means and Agglomerative Clustering, it is evident that the similarity between clusters is slacking. To ensure this obdervation is the case, silhouette score will be taken for both as well.\n\n**4.3.1 Create Functions to Evaluate Accuaracy, Precision, and Confusion Matrix**","metadata":{}},{"cell_type":"code","source":"import itertools\n# check permutation of label to predict whether a model predicts the clustering well\ndef permute_labels(ytrain, model_labels, n = 2):\n    # create some permutation for labels\n    permutations = itertools.permutations(range(n))\n\n    # create empty place to story permuations and accuracy score.\n    best_perm = None\n    best_acc = 0.0\n    best_precision = 0.0\n    \n    # iterate over the permutations; where p is label ordering\n    for p in permutations:\n        # permute the predicted labels\n        permuted_labels = [p[label] for label in model_labels]\n        # find accuracy between the permuted values and the expected y value\n        acc = accuracy_score(ytrain, permuted_labels)\n        prec = precision_score(ytrain, permuted_labels)\n\n        \n        # find best accuracy and permuation\n        if acc > best_acc:\n            best_acc = acc\n            best_prec = prec\n            best_perm = p\n            \n\n    return best_perm, best_acc, best_prec","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creat confusion matrix for the true labels and predicted labels\ndef create_conf_mx(y_train, model_labels, permutation = None):\n    # use when there is permuation\n    if permutation:\n        yt_labels = [permutation[i] for i in y_train]\n\n    # Create and return the confusion matrix\n    return confusion_matrix(yt_labels, model_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**4.3.2 Evaluate Metrics for K-Means** ","metadata":{}},{"cell_type":"code","source":"# find accuracy and label ordering/permutation\nkmeans_perm, kmeans_acc, kmeans_prec = permute_labels(y_train, kmeans_labels)\nprint(\"K-Means Clustering Label Ordering:\", kmeans_perm) \nprint(\"K-Means Clustering Accuracy:\", kmeans_acc)\nprint(\"K-Means Clustering Precision:\", kmeans_prec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create confusion matrix for k-means\nkm_mx_best = create_conf_mx(y_train, kmeans_labels, kmeans_perm)\nprint(\"Best True Values Confusion Matrix:\")\n\n# plot confusion matrix using sns\nplt.figure(figsize=(8, 6))\nsns.heatmap(km_mx_best, annot=True, cmap = \"Purples\", \n            xticklabels=[\"Non-Diabetic\", \"Diabetic\"], \n            yticklabels=[\"Non-Diabetic\", \"Diabetic\"])\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"Confusion Matrix for K-Means Clustering\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"km_score = silhouette_score(X_train_nmf, kmeans_labels)\nprint(\"Silhouette Score: \", km_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** The following K-Means Clustering Metrics were acquired: \n- Accuracy: 0.8052884615384616\n- Precision: 0.9666666666666667\n- Silhouette Score:  0.44072574501920675\n\nBased on these values, it is evident that the K-Means Clustering can some what accurately and precisely predict the groupings of who would be at risk for diabetes; however, the clusting method itself does not have a clear similarity between the points. This is demonstrated from how lowe the silhouette score is. \n\n**4.3.2 Evaluate Metrics for Hierarchical Clustering** ","metadata":{}},{"cell_type":"code","source":"# find accuracy and label ordering/permutation\nagg_perm, agg_acc, agg_prec = permute_labels(y_train, agg_labels)\n\n# print output\nprint(\"Hierarchical Clustering Label Ordering:\", agg_perm) \nprint(\"Hierarchical Clustering Accuracy:\", agg_acc)\nprint(\"Hierarchical Clustering Precision:\", agg_prec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create confusion matrix for hierarchical\nagg_mx_best = create_conf_mx(y_train, agg_labels, agg_perm)\nprint(\"Best True Values Confusion Matrix for Hierarchical Clustering:\")\nprint(agg_mx_best)\n\n# plot confusion matrix using sns\nplt.figure(figsize=(8, 6))\nsns.heatmap(agg_mx_best, annot=True, cmap = \"Purples\", \n            xticklabels=[\"Non-Diabetic\", \"Diabetic\"], \n            yticklabels=[\"Non-Diabetic\", \"Diabetic\"])\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"True Values\")\nplt.title(\"Confusion Matrix for Hierarchical Clustering\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"agg_score = silhouette_score(X_train_nmf, agg_labels)\nprint(\"Silhouette Score: \", agg_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Observations:** The following Hierarchical Clustering Metrics were acquired: \n- Accuracy: 0.8173076923076923\n- Precision: 0.9625668449197861\n- Silhouette Score: 0.37396613190254996\n\nBased on these values, similar to k-means clustering, Hierarchical Clustering can some what accurately and precisely predict the groupings of who would be at risk for diabetes; however, the clustering method ha no similarities found between the data points in the clusters. This confirms the observations frorm the Dendrogram. ","metadata":{}},{"cell_type":"markdown","source":"### 5 Results and Analysis\n\n---\n\n#### 5.1 Results\nThe accuracy scores and precision scores for K-Means Clustering was found to be 0.805 and 0.967; respectively, while Hierarchical Clustric was found to be 0.817 and 0.963; respectively.\nFurthermore the Confusion Matrices demonstrated that both models were somewhat able to predict True Positives and True Negatives more than the False Positives and False Negatives.\n\nDespite this potential numerically, the models for each showed their respective clusterings had many assumptions, especially for hierarchical clustering. When looking at the dendrogram, there was minimal simlarity between the clusterings, which is what cause the dendrogram to look really distorted and tall. As for K-means, in comparison to the Non-Negative Matrix plot, there were many data points that were misclassified. \n\nLastly, for the silhouette score, which evaluated the similarity score between clusters, K-means had a value of 0.441, while Hierarchical had a score of 0.374","metadata":{}},{"cell_type":"markdown","source":"\n\n#### 5.2 Analysis\nGiven the accuracy scores, precision scores, and confusion matrices for both the K-Means and Hierarchical Clustering have the potential to predict those at risk for diabetes, especially with accuracies greater than 80% and precisions greater than 90%. However, based on the models these cluering approaches display, there is a huge lack of smilarity between clusters that it does not actually cluster with knowledge. Furthermore, the silhouette scores for both models wer both below 0.5, indicating the similarities between clusters were minimal. Given this, although the models demonstrate potential to accurately and precisely predict, it does not predict in a clear way. This indicate that more work needs to be done in order to potentially predict accurately in the future.","metadata":{}},{"cell_type":"markdown","source":"### 6 Closing \n\n---\n\n#### 6.1 Things to Improve\nAlthough there is potential to continue utilizing either clustering method for clustering diabetes proned individuals by diabetes risk, there is definitely a need to reevaluate the approach for this type of unsupervised learning. One possible change that can be made is to simplify the data further via evalutaion of the confusion matrix. Furthermore, adjusting the models' hyperparameters could possible contribute to an improved model. Lastly, another suggestion would be to utilize other unsupervised learning or dimension reduction appraoches, in order to cater to the data set better.\n\n#### 6.2 Conclusion\nIn conclusion, it is evident that through the usage of K-Means adn Hierarchical Clustering that predicting who is diabetic or not is possible; however the model selection needs further work and investigation. Nonetheless, based on the evaluation of both models' metrics, all the features have the potential contribute towards predicting diabetes. Overall this unsupervised learning approach needs more work and adjustments and has the potential to predict, but the confidence to do so is not there quite yet. In conclusion, \"Gender\", \"Polyuria\", \"Polydipsia\", \"Sudden weight loss\", \"Weakness\", \"Polyphagia\", \"Genital thrush\", \"Visual blurring\", \"Itching\", \"Irritability\", \"Delayed healing\",\"Partial paresis\", \"Muscle stiffness\", \"Alopecia\", and \"Obesity\" are all potential risk predictors for diabetes via clustering, but the model needs work to allow for more confidence in predicting.","metadata":{}},{"cell_type":"markdown","source":"### 7 Source\n\n---\n\n**7.1 Citation**\n\nHimanshu (rcratos).(n.d.). Diabetes Risk Prediction [Data set]. Kaggle. Retrieved from https://www.kaggle.com/datasets/rcratos/diabetes-risk-prediction","metadata":{}},{"cell_type":"markdown","source":"### 8 Github\n\n---\n\n**GitHub Link:** https://github.com/kpnguyenco/Unsupervised-Data-Analysis-Diabetes-Risk.git","metadata":{}}]}